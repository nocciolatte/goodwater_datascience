# Winner Analysis Writeup
## Hypotheses
+ Intuition points to the differential between customer acquisition costs and customer lifetime value being one of the key drivers for e-commerce success. Since our example data set does not reveal anything about customer acquisition costs, we can choose to focus on the customer lifetime value (CLV) portion. Clearly we would want to maximize this.
    + CLV depends on both customer order value and order frequency - or in other words, retention.
    + Since we have benchmarking data on M1, M3, and M12 retention rates, we can focus on those in the larger dataset as well.
    + We construct a modified discounted CLV, using discount rate 10%, and incorporating the M1, M3, and M12 retention rates. We could choose to calculate the full CLV using all monthly retention rates for the broader dataset, but that would artificially inflate CLV values when compared to the benchmarking dataset just because we have more complete monthly retention data, so we choose to just use the features both datasets have in common.

+ Looking at the e-commerce company's health holistically, we would want to examine its revenues and costs. Our data set does not show anything about the company's operational costs, so we can choose to focus on revenue growth.
    + For especially younger companies, we would want to see a dramatic increase from sales one year ago, and also generally a modest but positive increase from that of one quarter ago (though exceptions might be made if the previous quarter happened to be a retail-heavy quarter like Q4, which we can keep in mind).
    + Let's construct two features, one called 1q_sales_pct_chg and one called 1y_sales_pct_chg, which denote the percentage sales growth between 1q and 1y ago, compared to current month.

## Model Considerations
+ Given the relatively limited amount of data, a simpler approach might be best. We are effectively going to be engineering features only and doing some comparisons on key data metrics. I personally feel that with this amount of data regressions would be prone to overfitting and unsupported extrapolation.
+ Usage: run main.py to see the descriptive stats and the classification of the broader sample companies.
    + Let's first come up with some interval boundaries for our comparisons. For the benchmarking companies, we calculate the mean and std of our three main features.
    + We then process the data for the broader dataset to construct our three features again, and then use them to classify the companies. We bucket by one std above mean, two std+ above mean, as well as one std below mean, and two std+ below mean.

## Insights
+ Because of the relative completeness of the data used in calculating CLV, we are able to tighten the parameters upon which we use to judge the companies in the broader dataset. We can require for a "winner" to achieve CLV above the mean CLV of the benchmark dataset, and still be able to see some companies pass the one std above mean test. Those that are able to achieve one std above mean are surpassing around 68% of all benchmarking companies (assuming Normal distribution), so that should be a very reassuring statistic. From running main.py, one can see that those companies that pass the CLV test are: **AMZN, CVS, Chatbooks, Crowdmade, FirstDibs, GenerationTux, Lot18, Lovepop, LowesFoods, LuckyVitamin, M, MVMTWatches, MilkMakeup, Petcube, Redbox, Reverb, StyleSeat, Vivino, Wolverine**, and those that strongly pass the CLV test are: **Crowdmade, MilkMakeup**. These companies are able to garner very promising lifetime sales from their customers, either by large order values or, more frequently, by good customer retention. At a glance, these are companies that either sell a variety of consumable products or services, meaning repeat customers are likely.

+ Comparing companies' on their sales growth trends seems a bit trickier. Intuitively, this is because every company, successful or not, can have fairly different sales growth trajectories that are less comparable to each other. However, it should be fair to say that they must all exhibit growth over the last year and last quarter, and should fall at least within the fist std confidence interval of the benchmarking companies. The yearly growth seemed to be wildly all over the place for quite a lot of our sample dataset. The quarterly appears more insightful, with some companies able to pass above the mean of that of the benchmarking companies.
    + A large concern here is that the dates of the sales numbers are quite mismatched between the benchmarking dataset and the broad sample dataset. Since e-commerce intuitively should be impacted by economic cycles, comparing stats from the early 2010s to that of the last two years seems suspect to me. More specific industry knowledge would be very helpful here to apply appropriate discount or inflation factors to attempt to align the two datasets in terms of economic condition.

+ When requiring that companies pass all three CLV and growth tests, we see that these companies pass: **AMZN, FirstDibs, GenerationTux, LowesFoods, M, MVMTWatches, Reverb, Vivino**. Intuitively AMZN doing well in all these tests, but not necessarily strongly passing all of them, makes sense, as it is no longer an extremely fast-growing startup but a stabler giant with exceptional customer retention. The rest of the list focus on specialty consumer goods, especially consumables, which could point to their lack of competition which allows them to have good growth and customer retention.

+ An easy extension of the model could be to apply a categorization of the goods/services that the companies specialize in as a classifier, and compare like with like to see which one could win out, rather than just a straight up comparison. (It might not make sense to compare makeup sellers to grocery stores, when the first is extremely high profit margin.)

+ As mentioned previously in the hypotheses section, other good metrics to incorporate quickly into the current model paradigm would be customer acquisition cost and company operational costs, so we get at a profitability measure rather than just sales revenue alone.

## Potential Extensions and Concerns
+ Feature engineering requires great domain-specific expertise and is can be an un-ending project in terms of continual improvement. Clearly with more thought and time better features could be constructed that are more relevant, especially if more data were available.
+ Deep learning methods can be helpful if we are presented with a much bigger dataset with much more features than is humanly possible to comb through, so that automatic feature extraction might be leveraged to come up with a model. Key considerations there are to avoid overfitting, and perhaps a concern with that approach would be that the process of extracting the features in this more automated way does not help us learn more about our data and gain domain knowledge, since it is done in a black-box manner. In any case, the main concern is how amenable our data would be to more sophisticated data analysis methods and to avoid overfitting.
+ There is some timing mismatch between the sales data and the retention data for the broader dataset. In order to extract sales data for current month, sales 1q ago, and sales 1y ago, we needed to assume most recent sales data reporting month. However, to get retention data, we needed to set the cohort month to be as early as possible to get more complete retention data, so in effect we might have been combining sales data valid for month 2018-06 with retention data valid for 2017-06. This can certainly be problematic if that was a key year for a company and deserves further investigation as a valid assumption. However, for the purposes of the exercise, we are allowing this time mismatch so as to be able to work with more data.
+ There is also timing mismatch between the benchmarking dataset and the sample dataset. Most of the data for the benchmark companies seem to be from 2011-2016, while the sample dataset has data from 2017-2018. By using one against the other, we are effectively saying that economic pressures have not significantly changed on the e-commerce industry during these years, which may not be a valid assumption. If possible, benchmarking data from more recent years would likely yield more precise results. Or if that is not possible, having a time-weighted rolling average as normalization for the older benchmarking data might make sense.
